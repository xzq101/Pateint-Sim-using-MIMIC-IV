"""
PatientSim AutoGen Evaluation Program
Based on llm_eval.py evaluation logic, provides evaluation for conversations generated by paitent_sim_autogen.py
"""

import os
import re
import sys
import json
import argparse
from typing import Dict, Any, List, Optional
from tqdm import tqdm
import autogen
from autogen import AssistantAgent

# Set UTF-8 encoding output
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

# ===== Evaluation Metrics Definition (4-point scale) =====
EVALUATION_METRICS = {
    "Personality": {
        "description": "Evaluate whether the patient consistently demonstrates their assigned personality traits",
        "criteria": [
            "0: Completely fails to demonstrate personality traits or severely inconsistent",
            "1: Occasionally demonstrates personality traits, but inconsistent",
            "2: Partially demonstrates personality traits with some consistency",
            "3: Mostly demonstrates personality traits with good consistency",
            "4: Perfectly demonstrates personality traits with excellent consistency"
        ]
    },
    "Language_Proficiency": {
        "description": "Evaluate whether the patient's language use matches their proficiency level",
        "criteria": [
            "0: Language use completely mismatches the set level",
            "1: Language use occasionally matches the level",
            "2: Language use partially matches the level",
            "3: Language use mostly matches the level",
            "4: Language use completely matches the level"
        ]
    },
    "Recall_Consistency": {
        "description": "Evaluate whether the patient's memory recall ability matches the setting",
        "criteria": [
            "0: Memory performance completely mismatches the setting",
            "1: Memory performance occasionally matches the setting",
            "2: Memory performance partially matches the setting",
            "3: Memory performance mostly matches the setting",
            "4: Memory performance completely matches the setting"
        ]
    },
    "Confusion_Consistency": {
        "description": "Evaluate whether the patient's confusion level matches the setting",
        "criteria": [
            "0: Confusion level completely mismatches the setting",
            "1: Confusion level occasionally matches the setting",
            "2: Confusion level partially matches the setting",
            "3: Confusion level mostly matches the setting",
            "4: Confusion level completely matches the setting"
        ]
    },
    "Clinical_Realism": {
        "description": "Evaluate whether the patient's symptom descriptions and responses are medically realistic",
        "criteria": [
            "0: Symptom descriptions are unrealistic or severely contradictory",
            "1: Symptom descriptions are basically realistic but have obvious issues",
            "2: Symptom descriptions are realistic but lack detail",
            "3: Symptom descriptions are realistic with good detail",
            "4: Symptom descriptions are highly realistic with rich detail"
        ]
    },
    "Response_Appropriateness": {
        "description": "Evaluate whether the patient's responses are relevant, natural, and reasonable",
        "criteria": [
            "0: Responses frequently off-topic or unreasonable",
            "1: Responses occasionally off-topic",
            "2: Responses basically on-topic",
            "3: Responses on-topic and natural",
            "4: Responses perfectly on-topic, natural, and reasonable"
        ]
    }
}

# ===== Sentence-Level Evaluation Prompts =====
SENTENCE_TYPE_CLASSIFIER_PROMPT = """
You are a helpful medical assistant. Your task is to classify each patient sentence into one of five types:
1. **politeness**: Greetings, thanks, apologies, etc.
2. **emotion**: Expressions of feelings, emotions, concerns.
3. **inquiry**: Questions asked by the patient.
4. **meta-information**: Comments about the conversation itself or communication difficulties.
5. **information**: Providing medical information, symptoms, history, or answering doctor's questions.

Based on the dialogue history and the current patient sentence, classify the sentence type.

Output format: {"sentence_type": "one of the five types above"}
"""

PROFILE_ITEM_IDENTIFIER_PROMPT = """
You are a helpful medical assistant. Your task is to determine whether each category of information from the patient's profile is mentioned in the patient's current utterance. Use the dialogue history as context.

For each category, output:
• '1' if the information is mentioned in the current utterance.
• '0' if it is not mentioned.

Additionally, provide a brief explanation for your decision. Please evaluate whether the following categories are relevant to the patient's current utterance: 'age', 'gender', 'race', 'tobacco', 'alcohol', 'illicit_drug', 'sexual_history', 'exercise', 'marital_status', 'children', 'living_situation', 'occupation', 'insurance', 'allergies', 'family_medical_history', 'medical_device', 'medical_history', 'present_illness', 'chief_complaint', 'pain', 'medication', 'arrival_transport', 'diagnosis'.

Output must be a list of valid JSON dictionaries, without any extra text, comments, or explanation. The output must be parseable by Python's json.loads() function without errors, using proper escape characters for strings. Each dictionary must follow this format:

[
  {
    "category": "name of the category (without any explanation)",
    "explanation": "Reason for the prediction",
    "prediction": 0 or 1
  },
  {
    "category": "age",
    "explanation": "The utterance 'I am 45 years old' mentions the patient's age.",
    "prediction": 1
  },
  {
    "category": "gender",
    "explanation": "The utterance does not mention the patient's gender.",
    "prediction": 0
  }
]
"""

FACTUAL_ACCURACY_VERIFIER_PROMPT = """
You are a helpful medical assistant. Your task is to evaluate whether a patient's current utterance is entailed, contradicted, or neither by each item in their medical profile. Also, generate an explanation for your answer. Focus on the information that is explicitly mentioned in the given profile. Use the dialogue history to understand the utterance's context.

The profile is provided as a list, where each item represents a distinct category of information.

For each profile item, output:
• '1': if the utterance is entailed by the profile.
• '0': if the utterance is neither entailed nor contradicted by the profile.
• '-1': if the utterance contradicts the profile.

Output must be a list of valid JSON dictionaries, without any extra text, comments, or explanation. The output must be parseable by Python's json.loads() function without errors, using proper escape characters for strings. Each dictionary must follow this format:

{
  "profile": "the original profile information",
  "explanation": "Reason for the prediction",
  "entailment_prediction": 1 or 0 or -1
}

Example output:
[
  {
    "profile": "Age: 30",
    "explanation": "The utterance 'I am 30 years old' matches the profile.",
    "entailment_prediction": 1
  },
  {
    "profile": "Gender: Female",
    "explanation": "The utterance does not mention gender.",
    "entailment_prediction": 0
  }
]
"""

# ===== Dialogue-Level Profile Extraction Prompts (Figure D13) =====
PROFILE_EXTRACTION_PROMPT = """
[System Prompt]
Instruction: You are an AI assistant designed to extract structured medical information from
a patient-doctor conversation. Your task is to analyze the conversation content and extract all
relevant information into predefined categories based on the patient's responses. Include only
information explicitly mentioned in the conversation, unless otherwise specified. Return the
extracted information in the following valid JSON format.

Field Definitions: {field_definition}

Output Format (JSON): {output_format}

Guidelines:
1. Extract each field from the entire conversation with complete accuracy.
2. Keep each field concise and keyword-based phrases without full sentences or narrative
descriptions.
3. Express information briefly, avoiding verbs, pronouns, or unnecessary words.
4. If a field contains multiple values, combine them into a single string separated by
semicolons.
5. Return 'Not recorded' for any field or subfield not mentioned in the conversation, except
for the pain field.
6. For the pain field, if patients do not explicitly state a score, predict the score (0–10)
based on their description and note it as predicted (e.g., '3 (predicted)').
7. Maintain the exact JSON structure without adding or removing fields.

[User Prompt]
Conversation: {conversation}
"""

# ===== Profile Consistency Evaluation Prompt (Figure D14) =====
PROFILE_CONSISTENCY_PROMPT = """
[System Prompt]
Instruction: You are a helpful medical assistant. Your task is to evaluate the consistency
between the Ground Truth (GT) and Prediction profile for each item. Also, generate an
explanation for your answer. The GT and Prediction are provided as dictionaries. For each
key, rate the consistency on a scale from 1 to 4, where:
• '4': The prediction contains the exact or semantically equivalent value for the GT.
• '3': The prediction contains a partially correct or semantically similar value for the GT.
• '2': The prediction contains only a small part of the value or a distantly related value for
the GT.
• '1': The prediction is completely incorrect compared to the GT.

Allow for differences in text expression if the meaning is the same or very similar, using
medical knowledge to assess semantic equivalence. Output must be a valid JSON object,
without any additional text or comments. The output JSON must be loadable using Python's
json.load() function with proper escape characters. The key of the output JSON must be the
key of the input GT dictionary, and the value must be a string formatted as '[REASON]: write
a brief feedback for criteria, [RESULT]: an integer number between 1 and 4'.

[User Prompt]
GT_profile: {profile_data}
Prediction_profile: {predict_dict}
"""

# ===== Persona-Aware Consistency Evaluation Prompt =====
PERSONA_AWARE_CONSISTENCY_PROMPT = """
[System Prompt]
Instruction: You are a helpful medical assistant specialized in evaluating patient simulation quality. 
Your task is to evaluate the consistency between the Ground Truth (GT) profile and the information 
extracted from dialogue (Prediction), while considering the patient's persona configuration.

Patient Persona Configuration:
- Personality: {personality}
- Language Proficiency: {language_proficiency}
- Recall Level: {recall_level}
- Confusion Level: {confusion_level}

CRITICAL: Consider how the persona affects information disclosure:
• Low recall → Patient may forget details, provide vague answers, or say "I don't remember"
• High confusion → Patient may provide contradictory or unclear information
• Pleasing personality → Patient may downplay problems or withhold sensitive information
• Agreeable personality → Patient may agree too quickly without full disclosure
• Language barriers → Patient may struggle to express complex medical history

Evaluation Task:
For each profile item, provide TWO separate evaluations:

1. **Information Extraction Score (1-4)**: How well was the information extracted from dialogue?
   - '4': Complete and accurate extraction
   - '3': Partial extraction with key details
   - '2': Minimal extraction, mostly missing
   - '1': Completely missing or incorrect

2. **Role-Play Consistency Score (1-4)**: Is the patient's incomplete/inaccurate disclosure appropriate given their persona?
   - '4': Patient behavior perfectly matches persona (e.g., low recall → vague answers)
   - '3': Patient behavior mostly matches persona
   - '2': Patient behavior partially matches persona
   - '1': Patient behavior contradicts persona (e.g., low recall but perfect memory)

Output Format:
Output must be a valid JSON object. The key must be the profile item name, and the value must 
be a string formatted as:
'[EXTRACTION_REASON]: <reason for extraction score>, [EXTRACTION_SCORE]: <1-4>, [ROLEPLAY_REASON]: <reason for roleplay score>, [ROLEPLAY_SCORE]: <1-4>'

Example output format (do not include this exact content in your response):
{{
  "alcohol": "[EXTRACTION_REASON]: Patient only mentioned 'occasional drinking', omitting alcohol dependence and Antabuse treatment. Information extraction is incomplete., [EXTRACTION_SCORE]: 2, [ROLEPLAY_REASON]: Given the pleasing personality and low recall, it is realistic for the patient to minimize alcohol problems and avoid mentioning sensitive treatment details., [ROLEPLAY_SCORE]: 4"
}}

[User Prompt]
GT_profile: {profile_data}
Prediction_profile: {predict_dict}
Persona_config: {persona_config}
"""

# ===== Evaluation Prompt Templates =====
EVALUATOR_SYSTEM_PROMPT = """
You are an expert medical simulation evaluator. Your task is to review doctor-patient conversations and provide objective, detailed evaluations.

For each evaluation dimension, you must:
1. Carefully analyze the conversation
2. Provide specific examples from the dialogue
3. Assign a score (0-4) based on the criteria
4. Give constructive feedback

Be fair, objective, and thorough in your evaluation. The scoring scale is 0-4, where 4 is the highest score.
"""

EVALUATION_USER_PROMPT_TEMPLATE = """
# Evaluation Task

## Patient Profile
- **Age**: {age}
- **Gender**: {gender}
- **Chief Complaint**: {chief_complaint}
- **Diagnosis**: {diagnosis}

## Patient Persona
- **Personality**: {personality}
- **Language Proficiency**: {language_proficiency}
- **Recall Level**: {recall_level}
- **Confusion Level**: {confusion_level}

## Conversation to Evaluate
{conversation}

## Evaluation Dimension: {dimension}

### Description
{description}

### Scoring Criteria
{criteria}

## Your Task
Please evaluate the patient's performance on the dimension **{dimension}**.

Provide your evaluation in the following format:

[ANALYSIS]
<Detailed analysis with specific examples from the conversation>

[SCORE]
<A single number from 0 to 4>

[FEEDBACK]
<Constructive feedback and suggestions>

[RESULT]: <Your final score (0-4)>
"""


# ===== Configuration Loading =====
def load_evaluator_config() -> Dict[str, Any]:
    """
    Load LLM configuration for evaluator
    Recommended to use more powerful models (e.g., GPT-4o) as evaluator
    """
    api_key = os.environ.get("OPENAI_API_KEY", "not-needed")
    
    config_list = [
        {
           # "model": "deepseek-r1:14b",  # or use gpt-4o, gemini-2.5-flash
            "model": "deepseek-r1:8b",  # or use gpt-4o, gemini-2.5-flash
            "api_key": api_key,
            "base_url": "http://localhost:11434/v1",
        }
    ]
    
    llm_config = {
        "config_list": config_list,
        "temperature": 0.0,  # Evaluator uses low temperature to ensure consistency
        "timeout": 120,
    }
    
    return llm_config


# ===== Conversation History Processing =====
def parse_txt_conversation(txt_content: str) -> Dict[str, Any]:
    """
    Parse txt format conversation file
    Return structured conversation data
    """
    lines = txt_content.split('\n')
    
    # Extract patient profile information
    patient_info = {}
    persona_info = {}
    messages = []
    
    in_conversation = False
    current_turn = None
    current_role = None
    current_content = []
    
    for line in lines:
        line = line.strip()
        
        # Extract Patient Profile section
        if line.startswith('Age:'):
            patient_info['age'] = line.split(':', 1)[1].strip()
        elif line.startswith('Gender:'):
            patient_info['gender'] = line.split(':', 1)[1].strip()
        elif line.startswith('Chief Complaint:'):
            patient_info['chief_complaint'] = line.split(':', 1)[1].strip()
        elif line.startswith('Medical History:'):
            patient_info['medical_history'] = line.split(':', 1)[1].strip()
        elif line.startswith('Current Medications:'):
            patient_info['medications'] = line.split(':', 1)[1].strip()
        
        # Extract Persona Configuration section
        elif line.startswith('Personality:'):
            persona_info['personality'] = line.split(':', 1)[1].strip()
        elif line.startswith('Language Proficiency:'):
            persona_info['language_proficiency'] = line.split(':', 1)[1].strip()
        elif line.startswith('Recall Level:'):
            persona_info['recall_level'] = line.split(':', 1)[1].strip()
        elif line.startswith('Confusion Level:'):
            persona_info['confusion_level'] = line.split(':', 1)[1].strip()
        
        # Conversation content starts
        elif 'Conversation Content:' in line:
            in_conversation = True
            continue
        
        # Parse conversation turns
        if in_conversation:
            turn_match = re.match(r'\[Turn \d+\] (Doctor|Patient):', line)
            if turn_match:
                # Save previous turn content
                if current_role and current_content:
                    messages.append({
                        'role': current_role,
                        'content': ' '.join(current_content).strip()
                    })
                
                # Start new turn
                current_role = turn_match.group(1)
                current_content = []
            elif line and current_role:
                # Continue current turn content
                current_content.append(line)
    
    # Save last turn
    if current_role and current_content:
        messages.append({
            'role': current_role,
            'content': ' '.join(current_content).strip()
        })
    
    return {
        'patient_profile': {
            'demographics': {
                'age': patient_info.get('age', 'Unknown'),
                'gender': patient_info.get('gender', 'Unknown')
            },
            'chief_complaint': patient_info.get('chief_complaint', 'Unknown'),
            'medical_history': patient_info.get('medical_history', ''),
            'medications': patient_info.get('medications', '')
        },
        'persona_config': persona_info,
        'messages': messages
    }


def load_conversation_from_txt_and_json(txt_file: str, json_file: str = "ed_patient_records_enriched.json") -> Dict[str, Any]:
    """
    Load complete conversation data from txt file and enriched JSON file
    
    Args:
        txt_file: simulation_outputsfolder txt file path
        json_file: enriched JSON file path
    
    Returns:
        Complete data containing patient information and conversation
    """
    # Extract patient_id from filename
    patient_id = os.path.splitext(os.path.basename(txt_file))[0]
    
    # Read txt conversation
    with open(txt_file, 'r', encoding='utf-8') as f:
        txt_content = f.read()
    
    conversation_data = parse_txt_conversation(txt_content)
    conversation_data['id'] = patient_id
    
    # Read complete patient information from enriched JSON
    if os.path.exists(json_file):
        with open(json_file, 'r', encoding='utf-8') as f:
            patient_records = json.load(f)
        
        # Match by patient_id
        for record in patient_records:
            if record['hadm_id'] == patient_id:
                # Supplement detailed patient information
                conversation_data['patient_profile']['diagnosis'] = record.get('diagnosis', 'Unknown')
                conversation_data['patient_profile']['allergies'] = record.get('allergies', '')
                conversation_data['patient_profile']['race'] = record.get('race', '')
                conversation_data['patient_profile']['full_record'] = record
                break
    
    return conversation_data


def load_conversation_history(conversation_file: str) -> List[Dict[str, Any]]:
    """
    Load conversation history from file
    
    Supports three formats:
    1. JSONL format (one conversation per line)
    2. JSON format (single conversation or conversation list)
    3. TXT format (txt files in simulation_outputs)
    """
    if not os.path.exists(conversation_file):
        raise FileNotFoundError(f"Conversation file does not exist: {conversation_file}")
    
    if conversation_file.endswith('.txt'):
        # TXT format - single conversation
        conversation_data = load_conversation_from_txt_and_json(conversation_file)
        return [conversation_data]
    
    with open(conversation_file, 'r', encoding='utf-8') as f:
        if conversation_file.endswith('.jsonl'):
            # JSONL format
            conversations = [json.loads(line) for line in f]
        else:
            # JSON format
            data = json.load(f)
            if isinstance(data, list):
                conversations = data
            else:
                conversations = [data]
    
    return conversations


def format_conversation(messages: List[Dict[str, str]]) -> str:
    """
    Format conversation history into readable text
    """
    formatted = ""
    for msg in messages:
        role = msg.get("role", "unknown").upper()
        content = msg.get("content", "")
        formatted += f"{role}: {content}\n\n"
    return formatted.strip()


# ===== Evaluator Agent =====
def create_evaluator_agent(llm_config: Dict[str, Any]) -> AssistantAgent:
    """
    Create evaluator Agent
    """
    evaluator = AssistantAgent(
        name="Evaluator",
        system_message=EVALUATOR_SYSTEM_PROMPT,
        llm_config=llm_config,
        human_input_mode="NEVER"
    )
    
    return evaluator


# ===== Evaluation Functions =====
def evaluate_single_dimension(
    evaluator: AssistantAgent,
    conversation: str,
    patient_profile: Dict[str, Any],
    persona_config: Dict[str, str],
    dimension: str,
    metric_info: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Evaluate single dimension
    """
    # Build evaluation prompt
    criteria_text = "\n".join(metric_info["criteria"])
    
    user_prompt = EVALUATION_USER_PROMPT_TEMPLATE.format(
        age=patient_profile['demographics']['age'],
        gender=patient_profile['demographics']['gender'],
        chief_complaint=patient_profile['chief_complaint'],
        diagnosis=patient_profile.get('diagnosis', 'Unknown'),
        personality=persona_config['personality'],
        language_proficiency=persona_config['language_proficiency'],
        recall_level=persona_config['recall_level'],
        confusion_level=persona_config['confusion_level'],
        conversation=conversation,
        dimension=dimension,
        description=metric_info['description'],
        criteria=criteria_text
    )
    
    # Call evaluator
    messages = [{"role": "user", "content": user_prompt}]
    
    try:
        response = evaluator.generate_reply(messages=messages)
        
        # Parse response
        result = {
            "dimension": dimension,
            "full_response": response,
            "score": None,
            "analysis": None,
            "feedback": None
        }
        
        # Extract score
        score_match = re.search(r'\[RESULT\]:\s*(\d+)', response)
        if score_match:
            result["score"] = int(score_match.group(1))
        else:
            # Alternative: extract from [SCORE] tag
            score_match = re.search(r'\[SCORE\]\s*(\d+)', response)
            if score_match:
                result["score"] = int(score_match.group(1))
        
        # Extract analysis
        analysis_match = re.search(r'\[ANALYSIS\](.*?)\[SCORE\]', response, re.DOTALL)
        if analysis_match:
            result["analysis"] = analysis_match.group(1).strip()
        
        # Extract feedback
        feedback_match = re.search(r'\[FEEDBACK\](.*?)(?:\[RESULT\]|$)', response, re.DOTALL)
        if feedback_match:
            result["feedback"] = feedback_match.group(1).strip()
        
        return result
        
    except Exception as e:
        print(f"Evaluating dimension {dimension} error occurred: {e}")
        return {
            "dimension": dimension,
            "error": str(e),
            "score": None
        }


def evaluate_conversation(
    conversation_data: Dict[str, Any],
    evaluator: AssistantAgent,
    metrics: Dict[str, Dict[str, Any]]
) -> Dict[str, Any]:
    """
    Evaluate complete conversation
    """
    # Extract information
    patient_profile = conversation_data.get("patient_profile", {})
    persona_config = conversation_data.get("persona_config", {})
    messages = conversation_data.get("messages", [])
    
    # Format conversation
    conversation_text = format_conversation(messages)
    
    # Evaluation results
    results = {
        "conversation_id": conversation_data.get("id", "unknown"),
        "patient_profile": patient_profile,
        "persona_config": persona_config,
        "evaluations": {},
        "overall_score": 0.0
    }
    
    # Evaluate each dimension
    scores = []
    for dimension, metric_info in metrics.items():
        # Skip non-applicable dimensions
        if dimension == "Confusion_Consistency" and persona_config.get("confusion_level") == "Normal":
            continue
        
        print(f"  Evaluating dimension: {dimension}...")
        eval_result = evaluate_single_dimension(
            evaluator,
            conversation_text,
            patient_profile,
            persona_config,
            dimension,
            metric_info
        )
        
        results["evaluations"][dimension] = eval_result
        
        if eval_result.get("score") is not None:
            scores.append(eval_result["score"])
    
    # Calculate total score
    if scores:
        results["overall_score"] = sum(scores) / len(scores)
    
    return results


# ===== Batch Evaluation =====
def batch_evaluate_from_simulation_folder(
    simulation_folder: str = "simulation_outputs",
    output_file: str = "evaluation_results.json",
    metrics: Optional[Dict[str, Dict[str, Any]]] = None
) -> None:
    """
    Batch evaluate all txt conversation files in simulation_outputs folder
    Automatically match patient information from enriched JSON
    
    Args:
        simulation_folder: Folder path containing txt conversation files
        output_file: Evaluation results output file path
        metrics: Evaluation metrics (optional)
    """
    if metrics is None:
        metrics = EVALUATION_METRICS
    
    # Get all txt files
    from pathlib import Path
    txt_files = list(Path(simulation_folder).glob("*.txt"))
    
    if not txt_files:
        print(f"No txt files found in {simulation_folder} folder")
        return
    
    print(f"\n{'='*80}")
    print(f"Starting batch evaluation of simulation_outputs conversation files")
    print(f"{'='*80}")
    print(f"\nFound {len(txt_files)} conversation file(s):")
    for txt_file in txt_files:
        print(f"  - {txt_file.name}")
    print(f"\nWill match patient information from ed_patient_records_enriched.json")
    print()
    
    # Load configuration
    print("Loading evaluator configuration...")
    llm_config = load_evaluator_config()
    evaluator = create_evaluator_agent(llm_config)
    
    # Evaluate
    all_results = []
    for i, txt_file in enumerate(tqdm(txt_files, desc="Evaluation progress")):
        patient_id = txt_file.stem
        print(f"\n{'='*80}")
        print(f"Evaluating conversation {i+1}/{len(txt_files)}")
        print(f"File source: {txt_file.absolute()}")
        print(f"Patient ID: {patient_id}")
        print(f"{'='*80}")
        
        try:
            # Load conversation data (automatically match enriched JSON)
            conv_data = load_conversation_from_txt_and_json(str(txt_file))
            
            # Display matched patient information
            patient_profile = conv_data.get('patient_profile', {})
            print(f"\n【Patient Basic Information】")
            print(f"  Age: {patient_profile.get('demographics', {}).get('age', 'N/A')}")
            print(f"  Gender: {patient_profile.get('demographics', {}).get('gender', 'N/A')}")
            print(f"  Chief complaint: {patient_profile.get('chief_complaint', 'N/A')}")
            print(f"  Diagnosis: {patient_profile.get('diagnosis', 'N/A')}")
            
            persona_config = conv_data.get('persona_config', {})
            print(f"\n【Persona Configuration】")
            print(f"  Personality: {persona_config.get('personality', 'N/A')}")
            print(f"  Language proficiency: {persona_config.get('language_proficiency', 'N/A')}")
            print(f"  Recall level: {persona_config.get('recall_level', 'N/A')}")
            print(f"  Confusion level: {persona_config.get('confusion_level', 'N/A')}")
            
            print(f"\nStart evaluation...")
            
            # Evaluate
            result = evaluate_conversation(conv_data, evaluator, metrics)
            all_results.append(result)
            
            # Generate separate evaluation file for each patient
            individual_output = f"simulation_outputs/{patient_id}_evaluation.json"
            with open(individual_output, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"✓ Individual evaluation result saved: {individual_output}")
            
            # Real-time save summary file
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(all_results, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"Processing file {txt_file.name} error occurred: {e}")
            continue
    
    # Statistical summary
    print("\n" + "="*80)
    print("Evaluation completed!")
    print("="*80)
    
    # Calculate average scores
    all_scores = [r["overall_score"] for r in all_results if r["overall_score"] > 0]
    if all_scores:
        avg_score = sum(all_scores) / len(all_scores)
        print(f"Average total score: {avg_score:.2f}/4.0")
    
    # Average scores by dimension
    dimension_scores = {}
    for result in all_results:
        for dim, eval_data in result["evaluations"].items():
            if eval_data.get("score"):
                if dim not in dimension_scores:
                    dimension_scores[dim] = []
                dimension_scores[dim].append(eval_data["score"])
    
    print("\nAverage scores by dimension:")
    for dim, scores in dimension_scores.items():
        avg = sum(scores) / len(scores)
        print(f"  {dim}: {avg:.2f}/4.0")
    
    print(f"\nDetailed results saved to: {output_file}")
    print(f"\nList of evaluated files:")
    for i, result in enumerate(all_results, 1):
        patient_id = result.get('conversation_id', 'unknown')
        overall_score = result.get('overall_score', 0)
        individual_file = f"simulation_outputs/{patient_id}_evaluation.json"
        print(f"  {i}. {patient_id}.txt - Total score: {overall_score:.2f}/4.0")
        print(f"     → Individual evaluation file: {individual_file}")


def batch_evaluate(
    conversation_file: str,
    output_file: str,
    metrics: Optional[Dict[str, Dict[str, Any]]] = None
) -> None:
    """
    Batch evaluate multiple conversations
    Supports JSON/JSONL/TXT formats
    """
    if metrics is None:
        metrics = EVALUATION_METRICS
    
    # Load configuration
    print("Loading evaluator configuration...")
    llm_config = load_evaluator_config()
    evaluator = create_evaluator_agent(llm_config)
    
    # Load conversation
    print(f"Loading conversation history: {conversation_file}")
    conversations = load_conversation_history(conversation_file)
    print(f"Loaded {len(conversations)} conversations")
    
    # Evaluate
    all_results = []
    for i, conv_data in enumerate(tqdm(conversations, desc="Evaluation progress")):
        print(f"\nEvaluating conversation {i+1}/{len(conversations)}")
        result = evaluate_conversation(conv_data, evaluator, metrics)
        all_results.append(result)
        
        # Real-time save
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    # Statistical summary
    print("\n" + "="*80)
    print("Evaluation completed!")
    print("="*80)
    
    # Calculate average scores
    all_scores = [r["overall_score"] for r in all_results if r["overall_score"] > 0]
    if all_scores:
        avg_score = sum(all_scores) / len(all_scores)
        print(f"Average total score: {avg_score:.2f}/4.0")
    
    # Average scores by dimension
    dimension_scores = {}
    for result in all_results:
        for dim, eval_data in result["evaluations"].items():
            if eval_data.get("score"):
                if dim not in dimension_scores:
                    dimension_scores[dim] = []
                dimension_scores[dim].append(eval_data["score"])
    
    print("\nAverage scores by dimension:")
    for dim, scores in dimension_scores.items():
        avg = sum(scores) / len(scores)
        print(f"  {dim}: {avg:.2f}/4.0")
    
    print(f"\nDetailed results saved to: {output_file}")


# ===== Single Conversation Evaluation (for testing) =====
def evaluate_single_conversation_from_autogen(
    patient_profile: Dict[str, Any],
    persona_config: Dict[str, str],
    messages: List[Dict[str, str]],
    output_file: Optional[str] = None
) -> Dict[str, Any]:
    """
    Evaluate single AutoGen conversation
    
    Args:
        patient_profile: Patient profile
        persona_config: Persona configuration
        messages: Conversation message list
        output_file: Output file path (optional)
    
    Returns:
        Evaluation results dictionary
    """
    # Build conversation data
    conversation_data = {
        "id": "single_eval",
        "patient_profile": patient_profile,
        "persona_config": persona_config,
        "messages": messages
    }
    
    # Load evaluator
    llm_config = load_evaluator_config()
    evaluator = create_evaluator_agent(llm_config)
    
    # Evaluate
    print("Start evaluation...")
    result = evaluate_conversation(conversation_data, evaluator, EVALUATION_METRICS)
    
    # Print results
    print("\n" + "="*80)
    print("Evaluation results")
    print("="*80)
    print(f"Total score: {result['overall_score']:.2f}/4.0")
    print("\nScores by dimension:")
    for dim, eval_data in result["evaluations"].items():
        score = eval_data.get("score", "N/A")
        print(f"  {dim}: {score}/4")
        if eval_data.get("feedback"):
            print(f"    Feedback: {eval_data['feedback'][:100]}...")
    
    # Save results
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"\nResults saved to: {output_file}")
    
    return result


# ===== Sentence-Level Evaluation Functions =====
def classify_sentence_type(
    evaluator: AssistantAgent,
    sentence: str,
    dialogue_history: str
) -> str:
    """
    Classify sentence type: politeness, emotion, inquiry, meta-information, information
    """
    prompt = f"""{SENTENCE_TYPE_CLASSIFIER_PROMPT}

Dialogue History:
{dialogue_history}

Current Patient Sentence:
{sentence}

Please classify the sentence type.
"""
    
    messages = [{"role": "user", "content": prompt}]
    response = evaluator.generate_reply(messages=messages)
    
    try:
        result = json.loads(response)
        return result.get("sentence_type", "information")
    except:
        return "information"


def identify_related_profile_items(
    evaluator: AssistantAgent,
    sentence: str,
    dialogue_history: str
) -> List[Dict[str, Any]]:
    """
    Identify patient profile items mentioned in sentence
    """
    prompt = f"""{PROFILE_ITEM_IDENTIFIER_PROMPT}

Dialogue History:
{dialogue_history}

Current Patient Utterance:
{sentence}

Please evaluate which profile categories are mentioned.
"""
    
    messages = [{"role": "user", "content": prompt}]
    response = evaluator.generate_reply(messages=messages)
    
    try:
        result = json.loads(response)
        return result if isinstance(result, list) else []
    except:
        return []


def verify_factual_accuracy(
    evaluator: AssistantAgent,
    sentence: str,
    dialogue_history: str,
    profile_items: Dict[str, Any],
    related_categories: List[str]
) -> List[Dict[str, Any]]:
    """
    Verify factual consistency of sentence with patient profile (NLI)
    Using 1 (entailment), 0 (neutral), -1 (contradiction)
    """
    # Build profile list
    profile_list = []
    for category in related_categories:
        if category in profile_items and profile_items[category]:
            profile_list.append(f"{category}: {profile_items[category]}")
    
    if not profile_list:
        return []
    
    prompt = f"""{FACTUAL_ACCURACY_VERIFIER_PROMPT}

Dialogue History:
{dialogue_history}

Patient's Current Utterance:
{sentence}

Profile Items to Evaluate:
{chr(10).join([f"- {item}" for item in profile_list])}

Please evaluate whether the utterance is entailed, contradicted, or neither by each profile item.
"""
    
    messages = [{"role": "user", "content": prompt}]
    response = evaluator.generate_reply(messages=messages)
    
    try:
        result = json.loads(response)
        return result if isinstance(result, list) else []
    except Exception as e:
        print(f"Warning: Failed to parse factual accuracy response: {e}")
        return []


def batch_evaluate_sentence_level(
    simulation_folder: str = "simulation_outputs",
    output_file: str = "sentence_level_evaluation.json"
) -> None:
    """
    Batch sentence-level evaluation for all conversations in folder
    
    Args:
        simulation_folder: Folder containing txt conversation files
        output_file: Output JSON file for aggregated results
    """
    from pathlib import Path
    
    txt_files = list(Path(simulation_folder).glob("*.txt"))
    
    if not txt_files:
        print(f"No txt files found in {simulation_folder} folder")
        return
    
    print(f"\n{'='*80}")
    print(f"Batch Sentence-Level Evaluation")
    print(f"{'='*80}")
    print(f"\nFound {len(txt_files)} conversation file(s)")
    
    # Load evaluator
    llm_config = load_evaluator_config()
    evaluator = create_evaluator_agent(llm_config)
    
    all_conversations = []
    
    for i, txt_file in enumerate(tqdm(txt_files, desc="Sentence-level evaluation progress")):
        patient_id = txt_file.stem
        print(f"\n{'='*60}")
        print(f"Processing {i+1}/{len(txt_files)}: {patient_id}")
        print(f"{'='*60}")
        
        try:
            # Load conversation data
            conv_data = load_conversation_from_txt_and_json(str(txt_file))
            
            if not conv_data or 'messages' not in conv_data:
                print(f"Error: Failed to load conversation data from {txt_file}")
                continue
            
            patient_profile = conv_data.get('patient_profile', {})
            messages = conv_data.get('messages', [])
            
            sentence_results = []
            dialogue_history = ""
            
            for turn_idx, msg in enumerate(messages):
                if msg['role'] == 'Patient':
                    sentence = msg['content']
                    
                    # Step 1: Classify sentence type
                    sentence_type = classify_sentence_type(evaluator, sentence, dialogue_history)
                    
                    result = {
                        "turn": turn_idx + 1,
                        "sentence": sentence,
                        "type": sentence_type,
                        "related_items": [],
                        "factual_accuracy": []
                    }
                    
                    # Step 2 & 3: If information type, identify related items and verify accuracy
                    if sentence_type == "information":
                        related_items = identify_related_profile_items(evaluator, sentence, dialogue_history)
                        result["related_items"] = related_items
                        
                        # Get mentioned categories
                        mentioned_categories = [item['category'] for item in related_items if item.get('prediction') == 1]
                        
                        if mentioned_categories:
                            # Verify factual accuracy using full_record
                            full_record = patient_profile.get('full_record', {})
                            accuracy = verify_factual_accuracy(
                                evaluator, 
                                sentence, 
                                dialogue_history,
                                full_record, 
                                mentioned_categories
                            )
                            result["factual_accuracy"] = accuracy
                    
                    sentence_results.append(result)
                
                # Update dialogue history
                speaker = "Doctor" if msg['role'] == 'Doctor' else "Patient"
                dialogue_history += f"{speaker}: {msg['content']}\n"
            
            # Calculate Entail(%) for this conversation
            info_sentences = [s for s in sentence_results if s['type'] == 'information']
            if info_sentences:
                sentences_with_entailment = sum(
                    1 for s in info_sentences 
                    if any(acc.get('entailment_prediction') == 1 for acc in s['factual_accuracy'])
                )
                entail_percentage = (sentences_with_entailment / len(info_sentences)) * 100
            else:
                entail_percentage = 0
            
            conversation_result = {
                "conversation_id": conv_data.get('id', patient_id),
                "patient_profile": patient_profile,
                "sentence_evaluations": sentence_results,
                "summary": {
                    "total_sentences": len(sentence_results),
                    "information_sentences": len(info_sentences),
                    "entail_percentage": round(entail_percentage, 2)
                }
            }
            
            all_conversations.append(conversation_result)
            
            # Save individual result
            individual_output = f"simulation_outputs/{patient_id}_sentence_eval.json"
            with open(individual_output, 'w', encoding='utf-8') as f:
                json.dump(conversation_result, f, ensure_ascii=False, indent=2)
            print(f"✓ Saved: {individual_output}")
            
        except Exception as e:
            print(f"✗ Error processing {txt_file.name}: {e}")
            import traceback
            traceback.print_exc()
            continue
    
    # Save aggregated results
    final_results = {
        "conversations": all_conversations,
        "summary": {
            "total_conversations": len(all_conversations),
            "total_sentences": sum(c['summary']['total_sentences'] for c in all_conversations),
            "total_information_sentences": sum(c['summary']['information_sentences'] for c in all_conversations)
        }
    }
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_results, f, ensure_ascii=False, indent=2)
    
    # Print summary
    print(f"\n{'='*80}")
    print(f"Batch Sentence-Level Evaluation Completed!")
    print(f"{'='*80}")
    print(f"\nEvaluated {len(all_conversations)} conversations")
    print(f"Total sentences: {final_results['summary']['total_sentences']}")
    print(f"Information sentences: {final_results['summary']['total_information_sentences']}")
    
    avg_entail = sum(c['summary']['entail_percentage'] for c in all_conversations) / len(all_conversations) if all_conversations else 0
    print(f"Average Entail(%): {avg_entail:.2f}%")
    
    print(f"\nResults saved to: {output_file}")


def evaluate_sentence_level(
    conversation_file: str,
    output_file: str = "sentence_level_evaluation.json"
) -> None:
    """
    Perform sentence-level evaluation on conversation
    """
    print("Starting Sentence-Level Evaluation...")
    
    # Load evaluator
    llm_config = load_evaluator_config()
    evaluator = create_evaluator_agent(llm_config)
    
    # Load conversations
    conversations = load_conversation_history(conversation_file)
    
    all_results = []
    
    for conv_data in tqdm(conversations, desc="Sentence-level evaluation"):
        patient_profile = conv_data.get('patient_profile', {})
        messages = conv_data.get('messages', [])
        
        sentence_results = []
        dialogue_history = ""
        
        for i, msg in enumerate(messages):
            if msg['role'] == 'Patient':
                sentence = msg['content']
                
                # Step 1: Classify sentence type
                sentence_type = classify_sentence_type(evaluator, sentence, dialogue_history)
                
                result = {
                    "turn": i + 1,
                    "sentence": sentence,
                    "type": sentence_type,
                    "related_items": [],
                    "factual_accuracy": []
                }
                
                # Step 2 & 3: If information type, identify related items and verify accuracy
                if sentence_type == "information":
                    related_items = identify_related_profile_items(evaluator, sentence, dialogue_history)
                    result["related_items"] = related_items
                    
                    # Get mentioned categories
                    mentioned_categories = [item['category'] for item in related_items if item.get('prediction') == 1]
                    
                    # Verify factual accuracy
                    if mentioned_categories:
                        full_record = patient_profile.get('full_record', {})
                        accuracy = verify_factual_accuracy(
                            evaluator, 
                            sentence, 
                            dialogue_history,
                            full_record, 
                            mentioned_categories
                        )
                        result["factual_accuracy"] = accuracy
                
                sentence_results.append(result)
            
            # Update dialogue history
            dialogue_history += f"{msg['role']}: {msg['content']}\n"
        
        # Calculate Entail(%) according to Equation 2
        info_sentences = [r for r in sentence_results if r['type'] == 'information']
        entailed_sentences = []
        
        for sent_result in info_sentences:
            # Check if at least one profile item entails this sentence
            # max_k r_k · 1[NLI(s_tm, x_k) = entail]
            has_entailment = False
            for accuracy in sent_result.get('factual_accuracy', []):
                if accuracy.get('entailment_prediction') == 1:
                    has_entailment = True
                    break
            
            if has_entailment:
                entailed_sentences.append(sent_result)
        
        entail_percentage = (len(entailed_sentences) / len(info_sentences) * 100) if info_sentences else 0
        
        all_results.append({
            "conversation_id": conv_data.get('id', 'unknown'),
            "sentence_evaluations": sentence_results,
            "factual_accuracy_metrics": {
                "total_information_sentences": len(info_sentences),
                "entailed_sentences": len(entailed_sentences),
                "entail_percentage": round(entail_percentage, 2),
                "formula": "Entail(%) = (# info sentences with at least one entailment) / (# total info sentences) × 100"
            }
        })
    
    # Calculate overall statistics
    total_info_sentences = sum(r['factual_accuracy_metrics']['total_information_sentences'] for r in all_results)
    total_entailed = sum(r['factual_accuracy_metrics']['entailed_sentences'] for r in all_results)
    overall_entail_pct = (total_entailed / total_info_sentences * 100) if total_info_sentences > 0 else 0
    
    final_output = {
        "conversations": all_results,
        "overall_statistics": {
            "total_conversations": len(all_results),
            "total_information_sentences": total_info_sentences,
            "total_entailed_sentences": total_entailed,
            "overall_entail_percentage": round(overall_entail_pct, 2),
            "description": "Entail(%): Percentage of information sentences that are factually accurate (entailed by at least one profile item)"
        }
    }
    
    # Save results
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(final_output, f, ensure_ascii=False, indent=2)
    
    print(f"\nSentence-level evaluation results saved to: {output_file}")
    print(f"\n{'='*60}")
    print(f"Factual Accuracy Results (Equation 2):")
    print(f"{'='*60}")
    print(f"Total Information Sentences: {total_info_sentences}")
    print(f"Entailed Sentences: {total_entailed}")
    print(f"Entail(%): {overall_entail_pct:.2f}%")
    print(f"{'='*60}\n")


# ===== Dialogue-Level Profile Extraction and Consistency Evaluation =====
def extract_profile_from_dialogue(evaluator: AssistantAgent, conversation: str, field_definition: str, output_format: str) -> Dict[str, Any]:
    """
    Extract patient profile from dialogue using Figure D13 prompt
    
    Args:
        evaluator: LLM evaluator agent
        conversation: Full conversation text
        field_definition: JSON schema field definitions
        output_format: Expected JSON output format
    
    Returns:
        Extracted profile as dictionary
    """
    prompt = PROFILE_EXTRACTION_PROMPT.format(
        field_definition=field_definition,
        output_format=output_format,
        conversation=conversation
    )
    
    response = evaluator.generate_reply(messages=[{"role": "user", "content": prompt}])
    
    # Parse JSON response
    try:
        # Try to extract JSON from response
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            extracted_profile = json.loads(json_match.group())
            return extracted_profile
        else:
            print(f"Warning: Could not extract JSON from response")
            return {}
    except json.JSONDecodeError as e:
        print(f"Warning: Failed to parse extracted profile: {e}")
        return {}


def evaluate_profile_consistency(evaluator: AssistantAgent, gt_profile: Dict[str, Any], 
                                 predicted_profile: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """
    Evaluate consistency between ground truth profile and predicted profile using Figure D14 prompt
    
    Args:
        evaluator: LLM evaluator agent
        gt_profile: Ground truth patient profile
        predicted_profile: Extracted/predicted patient profile
    
    Returns:
        Dictionary mapping each profile key to consistency score and reasoning
    """
    prompt = PROFILE_CONSISTENCY_PROMPT.format(
        profile_data=json.dumps(gt_profile, ensure_ascii=False, indent=2),
        predict_dict=json.dumps(predicted_profile, ensure_ascii=False, indent=2)
    )
    
    response = evaluator.generate_reply(messages=[{"role": "user", "content": prompt}])
    
    # Parse JSON response
    try:
        # Try to extract JSON from response
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            consistency_results = json.loads(json_match.group())
            
            # Parse each result string to extract REASON and RESULT
            parsed_results = {}
            for key, value in consistency_results.items():
                reason_match = re.search(r'\[REASON\]:\s*(.+?)\s*\[RESULT\]:\s*(\d+)', value, re.DOTALL)
                if reason_match:
                    parsed_results[key] = {
                        "reason": reason_match.group(1).strip(),
                        "score": int(reason_match.group(2))
                    }
                else:
                    parsed_results[key] = {
                        "reason": value,
                        "score": 0
                    }
            
            return parsed_results
        else:
            print(f"Warning: Could not extract JSON from response")
            return {}
    except json.JSONDecodeError as e:
        print(f"Warning: Failed to parse consistency results: {e}")
        return {}


def evaluate_persona_aware_consistency(evaluator: AssistantAgent, gt_profile: Dict[str, Any], 
                                       predicted_profile: Dict[str, Any],
                                       persona_config: Dict[str, Any]) -> Dict[str, Dict[str, Any]]:
    """
    Evaluate consistency with persona awareness - separates extraction quality from roleplay quality
    
    Args:
        evaluator: LLM evaluator agent
        gt_profile: Ground truth patient profile
        predicted_profile: Extracted/predicted patient profile
        persona_config: Patient persona configuration (personality, recall, confusion, language)
    
    Returns:
        Dictionary with dual scoring: extraction_score and roleplay_score for each item
    """
    prompt = PERSONA_AWARE_CONSISTENCY_PROMPT.format(
        profile_data=json.dumps(gt_profile, ensure_ascii=False, indent=2),
        predict_dict=json.dumps(predicted_profile, ensure_ascii=False, indent=2),
        persona_config=json.dumps(persona_config, ensure_ascii=False, indent=2),
        personality=persona_config.get('personality', 'normal'),
        language_proficiency=persona_config.get('language_proficiency', 'native'),
        recall_level=persona_config.get('recall', 'normal'),
        confusion_level=persona_config.get('confusion', 'normal')
    )
    
    response = evaluator.generate_reply(messages=[{"role": "user", "content": prompt}])
    
    # Parse JSON response
    try:
        json_match = re.search(r'\{.*\}', response, re.DOTALL)
        if json_match:
            consistency_results = json.loads(json_match.group())
            
            # Parse each result string to extract dual scores
            parsed_results = {}
            for key, value in consistency_results.items():
                # Extract extraction score
                extraction_match = re.search(
                    r'\[EXTRACTION_REASON\]:\s*(.+?)\s*\[EXTRACTION_SCORE\]:\s*(\d+)', 
                    value, re.DOTALL
                )
                # Extract roleplay score
                roleplay_match = re.search(
                    r'\[ROLEPLAY_REASON\]:\s*(.+?)\s*\[ROLEPLAY_SCORE\]:\s*(\d+)', 
                    value, re.DOTALL
                )
                
                if extraction_match and roleplay_match:
                    parsed_results[key] = {
                        "extraction_reason": extraction_match.group(1).strip(),
                        "extraction_score": int(extraction_match.group(2)),
                        "roleplay_reason": roleplay_match.group(1).strip(),
                        "roleplay_score": int(roleplay_match.group(2))
                    }
                else:
                    # Fallback to single score if format not matched
                    parsed_results[key] = {
                        "extraction_reason": value,
                        "extraction_score": 0,
                        "roleplay_reason": "Parse error",
                        "roleplay_score": 0
                    }
            
            return parsed_results
        else:
            print(f"Warning: Could not extract JSON from response")
            return {}
    except json.JSONDecodeError as e:
        print(f"Warning: Failed to parse consistency results: {e}")
        return {}


def batch_dialogue_level_evaluation(
    simulation_folder: str = "simulation_outputs",
    output_file: str = "dialogue_evaluation_results.json",
    persona_aware: bool = True
) -> None:
    """
    Batch dialogue-level evaluation: extract profiles and evaluate consistency for all conversations
    
    Args:
        simulation_folder: Folder containing txt conversation files
        output_file: Output JSON file for aggregated results
        persona_aware: If True, use dual-scoring; if False, use simple consistency
    """
    from pathlib import Path
    
    txt_files = list(Path(simulation_folder).glob("*.txt"))
    
    if not txt_files:
        print(f"No txt files found in {simulation_folder} folder")
        return
    
    print(f"\n{'='*80}")
    print(f"Batch Dialogue-Level Evaluation")
    print(f"Mode: {'Persona-Aware (Dual Scoring)' if persona_aware else 'Simple Consistency'}")
    print(f"{'='*80}")
    print(f"\nFound {len(txt_files)} conversation file(s)")
    
    # Load evaluator
    llm_config = load_evaluator_config()
    evaluator = AssistantAgent(
        name="ProfileEvaluator",
        llm_config=llm_config,
        system_message="You are an expert medical information extraction assistant."
    )
    
    all_results = []
    
    for i, txt_file in enumerate(tqdm(txt_files, desc="Dialogue evaluation progress")):
        patient_id = txt_file.stem
        print(f"\n{'='*60}")
        print(f"Processing {i+1}/{len(txt_files)}: {patient_id}")
        print(f"{'='*60}")
        
        try:
            # Single file dialogue-level evaluation
            result = _evaluate_single_dialogue(
                str(txt_file),
                evaluator,
                persona_aware=persona_aware
            )
            
            if result:
                all_results.append(result)
                
                # Save individual result
                individual_output = f"simulation_outputs/{patient_id}_dialogue_eval.json"
                with open(individual_output, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print(f"✓ Saved: {individual_output}")
                
        except Exception as e:
            print(f"✗ Error processing {txt_file.name}: {e}")
            continue
    
    # Save aggregated results
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, ensure_ascii=False, indent=2)
    
    # Print summary statistics
    print(f"\n{'='*80}")
    print(f"Batch Dialogue-Level Evaluation Completed!")
    print(f"{'='*80}")
    
    if persona_aware:
        extraction_scores = []
        roleplay_scores = []
        for result in all_results:
            summary = result.get('summary', {})
            extraction_scores.append(summary.get('average_extraction_score', 0))
            roleplay_scores.append(summary.get('average_roleplay_score', 0))
        
        avg_extraction = sum(extraction_scores) / len(extraction_scores) if extraction_scores else 0
        avg_roleplay = sum(roleplay_scores) / len(roleplay_scores) if roleplay_scores else 0
        
        print(f"\nAverage Extraction Score: {avg_extraction:.2f}/4.0")
        print(f"Average Role-Play Score: {avg_roleplay:.2f}/4.0")
    else:
        consistency_scores = [r.get('summary', {}).get('average_consistency_score', 0) for r in all_results]
        avg_consistency = sum(consistency_scores) / len(consistency_scores) if consistency_scores else 0
        print(f"\nAverage Consistency Score: {avg_consistency:.2f}/4.0")
    
    print(f"\nEvaluated {len(all_results)} conversations")
    print(f"Results saved to: {output_file}")


def _evaluate_single_dialogue(conversation_file: str, evaluator, persona_aware: bool = True) -> Dict:
    """
    Helper function to evaluate a single conversation for dialogue-level metrics
    Returns structured result dictionary
    """
    # Load conversation and patient profile
    conv_data = load_conversation_from_txt_and_json(conversation_file)
    
    if not conv_data or 'messages' not in conv_data:
        print("Error: Failed to load conversation data")
        return None
    
    # Get ground truth profile
    gt_profile = conv_data.get('patient_profile', {})
    persona_config = conv_data.get('persona_config', {})
    
    # Format conversation for extraction
    conversation_text = ""
    for msg in conv_data['messages']:
        speaker = "Doctor" if msg['role'] == 'Doctor' else "Patient"
        conversation_text += f"{speaker}: {msg['content']}\n"
    
    # Define field definitions and output format
    field_definition = json.dumps({
        "age": "Patient's age in years",
        "gender": "Patient's gender",
        "race": "Patient's race/ethnicity",
        "chief_complaint": "Main reason for visit",
        "diagnosis": "Medical diagnosis",
        "present_illness": "History of present illness",
        "pain": "Pain level (0-10 scale)",
        "medical_history": "Past medical history",
        "medication": "Current medications",
        "allergies": "Known allergies",
        "family_medical_history": "Family medical history",
        "tobacco": "Tobacco use history",
        "alcohol": "Alcohol use history",
        "occupation": "Patient's occupation",
        "marital_status": "Marital status",
        "living_situation": "Living arrangement"
    }, ensure_ascii=False, indent=2)
    
    output_format = json.dumps(gt_profile, ensure_ascii=False, indent=2)
    
    # Extract profile from dialogue
    extracted_profile = extract_profile_from_dialogue(evaluator, conversation_text, field_definition, output_format)
    
    # Evaluate consistency
    if persona_aware:
        consistency_results = evaluate_persona_aware_consistency(evaluator, gt_profile, extracted_profile, persona_config)
        
        extraction_scores = [r['extraction_score'] for r in consistency_results.values() if 'extraction_score' in r]
        roleplay_scores = [r['roleplay_score'] for r in consistency_results.values() if 'roleplay_score' in r]
        
        avg_extraction = sum(extraction_scores) / len(extraction_scores) if extraction_scores else 0
        avg_roleplay = sum(roleplay_scores) / len(roleplay_scores) if roleplay_scores else 0
        
        result = {
            "conversation_id": conv_data.get('id', 'unknown'),
            "evaluation_mode": "persona_aware",
            "persona_config": persona_config,
            "ground_truth_profile": gt_profile,
            "extracted_profile": extracted_profile,
            "consistency_evaluation": consistency_results,
            "summary": {
                "average_extraction_score": round(avg_extraction, 2),
                "average_roleplay_score": round(avg_roleplay, 2),
                "total_items_evaluated": len(extraction_scores)
            }
        }
    else:
        consistency_results = evaluate_profile_consistency(evaluator, gt_profile, extracted_profile)
        
        scores = [result['score'] for result in consistency_results.values() if 'score' in result]
        avg_score = sum(scores) / len(scores) if scores else 0
        
        result = {
            "conversation_id": conv_data.get('id', 'unknown'),
            "evaluation_mode": "simple_consistency",
            "ground_truth_profile": gt_profile,
            "extracted_profile": extracted_profile,
            "consistency_evaluation": consistency_results,
            "summary": {
                "average_consistency_score": round(avg_score, 2),
                "total_items_evaluated": len(scores)
            }
        }
    
    return result


def dialogue_level_evaluation(conversation_file: str, output_file: str, persona_aware: bool = True):
    """
    Perform dialogue-level evaluation: extract profile from dialogue and compare with ground truth
    
    Args:
        conversation_file: Path to conversation file (txt format)
        output_file: Path to save evaluation results
        persona_aware: If True, use dual-scoring (extraction + roleplay); if False, use simple consistency
    """
    print(f"\n{'='*60}")
    print(f"Dialogue-Level Evaluation: Profile Extraction & Consistency")
    if persona_aware:
        print(f"Mode: Persona-Aware (Dual Scoring)")
    else:
        print(f"Mode: Simple Consistency")
    print(f"{'='*60}\n")
    
    # Load LLM evaluator
    llm_config = load_evaluator_config()
    evaluator = AssistantAgent(
        name="ProfileEvaluator",
        llm_config=llm_config,
        system_message="You are an expert medical information extraction assistant."
    )
    
    # Use helper function to do evaluation
    print("Extracting profile from dialogue...")
    print("Evaluating profile consistency...")
    
    results = _evaluate_single_dialogue(conversation_file, evaluator, persona_aware=persona_aware)
    
    if not results:
        print("Error: Failed to evaluate dialogue")
        return
    
    # Print results
    print(f"\n{'='*60}")
    print(f"Dialogue-level evaluation completed!")
    
    if persona_aware:
        summary = results.get('summary', {})
        avg_extraction = summary.get('average_extraction_score', 0)
        avg_roleplay = summary.get('average_roleplay_score', 0)
        total_items = summary.get('total_items_evaluated', 0)
        
        print(f"Average Extraction Score: {avg_extraction:.2f}/4 (Information Completeness)")
        print(f"Average Role-Play Score: {avg_roleplay:.2f}/4 (Persona Consistency)")
        print(f"Total Items Evaluated: {total_items}")
        print(f"\nInterpretation:")
        print(f"  - Low extraction + High roleplay = Realistic patient (e.g., low recall)")
        print(f"  - High extraction + High roleplay = Ideal simulation")
        print(f"  - Low extraction + Low roleplay = Poor simulation quality")
    else:
        summary = results.get('summary', {})
        avg_score = summary.get('average_consistency_score', 0)
        total_items = summary.get('total_items_evaluated', 0)
        
        print(f"Average Consistency Score: {avg_score:.2f}/4")
        print(f"Total Items Evaluated: {total_items}")
    
    # Save results
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"Results saved to: {output_file}")
    print(f"{'='*60}\n")


# ===== Command Line Interface =====
def main():
    parser = argparse.ArgumentParser(description="PatientSim AutoGen Conversation Evaluation Tool")
    
    parser.add_argument(
        "--conversation_file",
        type=str,
        default=None,
        help="Conversation history file path (JSON/JSONL/TXT format)"
    )
    
    parser.add_argument(
        "--simulation_folder",
        type=str,
        default=None,
        help="simulation_outputs folder path (batch evaluate all txt files)"
    )
    
    parser.add_argument(
        "--output_file",
        type=str,
        default="evaluation_results.json",
        help="Evaluation resultsoutput file path"
    )
    
    parser.add_argument(
        "--metrics",
        type=str,
        default=None,
        help="Custom evaluation metrics JSON file path (optional)"
    )
    
    parser.add_argument(
        "--sentence_level",
        action="store_true",
        help="Execute sentence-level evaluation"
    )
    
    parser.add_argument(
        "--dialogue_level",
        action="store_true",
        help="Execute dialogue-level profile extraction and consistency evaluation"
    )
    
    parser.add_argument(
        "--persona_aware",
        action="store_true",
        default=True,
        help="Use persona-aware dual scoring (extraction + roleplay) for dialogue-level evaluation (default: True)"
    )
    
    parser.add_argument(
        "--simple_consistency",
        action="store_true",
        help="Use simple consistency scoring (ignore persona) for dialogue-level evaluation"
    )
    
    args = parser.parse_args()
    
    # Load custom metrics (if provided)
    metrics = EVALUATION_METRICS
    if args.metrics:
        with open(args.metrics, 'r', encoding='utf-8') as f:
            metrics = json.load(f)
    
    # Dialogue-level evaluation (profile extraction and consistency)
    if args.dialogue_level:
        # Determine evaluation mode
        persona_aware = not args.simple_consistency  # Default True unless --simple_consistency is set
        
        if args.conversation_file:
            # Single file dialogue-level evaluation
            dialogue_level_evaluation(args.conversation_file, args.output_file, persona_aware=persona_aware)
        elif args.simulation_folder:
            # Batch dialogue-level evaluation
            print("Batch dialogue-level evaluation for all conversations...")
            batch_dialogue_level_evaluation(
                simulation_folder=args.simulation_folder,
                output_file=args.output_file,
                persona_aware=persona_aware
            )
        else:
            # Default: evaluate simulation_outputs folder
            if os.path.exists("simulation_outputs"):
                print("No file specified, will evaluate all conversations in simulation_outputs folder...")
                batch_dialogue_level_evaluation(
                    simulation_folder="simulation_outputs",
                    output_file=args.output_file,
                    persona_aware=persona_aware
                )
            else:
                parser.error("--dialogue_level requires --conversation_file or --simulation_folder")
        return
    
    # Sentence-level evaluation
    if args.sentence_level:
        if args.conversation_file:
            # Single file sentence-level evaluation
            evaluate_sentence_level(args.conversation_file, args.output_file)
        elif args.simulation_folder:
            # Batch sentence-level evaluation
            print("Batch sentence-level evaluation for all conversations...")
            batch_evaluate_sentence_level(
                simulation_folder=args.simulation_folder,
                output_file=args.output_file
            )
        else:
            # Default: evaluate simulation_outputs folder
            if os.path.exists("simulation_outputs"):
                print("No file specified, will evaluate all conversations in simulation_outputs folder...")
                batch_evaluate_sentence_level(
                    simulation_folder="simulation_outputs",
                    output_file=args.output_file
                )
            else:
                parser.error("--sentence_level requires --conversation_file or --simulation_folder")
        return
    
    # Batch evaluation (conversation-level)
    if args.simulation_folder:
        # Evaluate all txt files in simulation_outputs folder
        batch_evaluate_from_simulation_folder(
            simulation_folder=args.simulation_folder,
            output_file=args.output_file,
            metrics=metrics
        )
    elif args.conversation_file:
        # Evaluate single file
        batch_evaluate(
            conversation_file=args.conversation_file,
            output_file=args.output_file,
            metrics=metrics
        )
    else:
        # Default: evaluate simulation_outputs folder in current directory
        if os.path.exists("simulation_outputs"):
            print("No file specified, will evaluate all conversations in simulation_outputs folder...")
            batch_evaluate_from_simulation_folder(
                simulation_folder="simulation_outputs",
                output_file=args.output_file,
                metrics=metrics
            )
        else:
            parser.error("Please specify --conversation_file or --simulation_folder parameter")


if __name__ == "__main__":
    # If run directly, use command line arguments
    import sys
    if len(sys.argv) > 1:
        main()
    else:
        # Test mode: evaluate example conversation
        print("Test mode: evaluate example conversation")
        
        # Example data
        test_profile = {
            "demographics": {
                "age": 77,
                "gender": "Female"
            },
            "chief_complaint": "Dyspnea, Cough",
            "diagnosis": "Pneumonia"
        }
        
        test_persona = {
            "personality": "Overanxious",
            "language_proficiency": "Basic",
            "recall_level": "High",
            "confusion_level": "Normal"
        }
        
        test_messages = [
            {"role": "Doctor", "content": "Hello, what brings you here today?"},
            {"role": "Patient", "content": "I... I can't breathe good! Very scared!"},
            {"role": "Doctor", "content": "When did this start?"},
            {"role": "Patient", "content": "Maybe two days? Get worse yesterday."}
        ]
        
        evaluate_single_conversation_from_autogen(
            patient_profile=test_profile,
            persona_config=test_persona,
            messages=test_messages,
            output_file="test_evaluation.json"
        )

